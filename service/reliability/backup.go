package reliability

import (
	"bytes"
	"compress/gzip"
	"encoding/json"
	"errors"
	"fmt"
	"io"
	"os"
	"path/filepath"
	"sync"
	"time"
)

// BackupType å¤‡ä»½ç±»å‹
type BackupType int

const (
	BackupTypeFull BackupType = iota
	BackupTypeIncremental
	BackupTypeDifferential
)

// BackupStatus å¤‡ä»½çŠ¶æ€?
type BackupStatus int

const (
	BackupStatusPending BackupStatus = iota
	BackupStatusRunning
	BackupStatusCompleted
	BackupStatusFailed
)

// BackupMetadata å¤‡ä»½å…ƒæ•°æ?
type BackupMetadata struct {
	ID          string
	Type        BackupType
	Status      BackupStatus
	StartTime   time.Time
	EndTime     time.Time
	Tables      []string
	RecordCount int
	Size        int64
	Checksum    string
	FilePath    string
	Error       string
}

// BackupManager å¤‡ä»½ç®¡ç†å™?
type BackupManager struct {
	metadata     map[string]*BackupMetadata
	backups      []string
	metadataLock sync.RWMutex
	backupDir    string
}

// NewBackupManager åˆ›å»ºå¤‡ä»½ç®¡ç†å™?
func NewBackupManager(backupDir string) *BackupManager {
	// ç¡®ä¿å¤‡ä»½ç›®å½•å­˜åœ¨
	os.MkdirAll(backupDir, 0755)

	return &BackupManager{
		metadata:  make(map[string]*BackupMetadata),
		backups:   make([]string, 0),
		backupDir: backupDir,
	}
}

// Backup å¤‡ä»½æ•°æ®
func (bm *BackupManager) Backup(backupType BackupType, tables []string, data interface{}) (string, error) {
	metadata := &BackupMetadata{
		ID:        generateBackupID(),
		Type:      backupType,
		Status:    BackupStatusRunning,
		StartTime: time.Now(),
		Tables:    tables,
	}

	bm.metadataLock.Lock()
	bm.metadata[metadata.ID] = metadata
	bm.metadataLock.Unlock()

	// åºåˆ—åŒ–æ•°æ?
	jsonData, err := json.Marshal(data)
	if err != nil {
		metadata.Status = BackupStatusFailed
		metadata.Error = err.Error()
		metadata.EndTime = time.Now()
		return metadata.ID, err
	}

	// å‹ç¼©æ•°æ®
	compressed, err := compressData(jsonData)
	if err != nil {
		metadata.Status = BackupStatusFailed
		metadata.Error = err.Error()
		metadata.EndTime = time.Now()
		return metadata.ID, err
	}

	// å†™å…¥æ–‡ä»¶
	filePath := filepath.Join(bm.backupDir, fmt.Sprintf("%s.backup.gz", metadata.ID))
	err = os.WriteFile(filePath, compressed, 0644)
	if err != nil {
		metadata.Status = BackupStatusFailed
		metadata.Error = err.Error()
		metadata.EndTime = time.Now()
		return metadata.ID, err
	}

	// æ›´æ–°å…ƒæ•°æ?
	metadata.Status = BackupStatusCompleted
	metadata.EndTime = time.Now()
	metadata.Size = int64(len(compressed))
	metadata.FilePath = filePath
	metadata.RecordCount = calculateRecordCount(data)

	// è®¡ç®—æ ¡éªŒå’?
	metadata.Checksum = calculateChecksum(compressed)

	bm.metadataLock.Lock()
	bm.backups = append(bm.backups, metadata.ID)
	bm.metadataLock.Unlock()

	return metadata.ID, nil
}

// Restore æ¢å¤æ•°æ®
func (bm *BackupManager) Restore(backupID string, data interface{}) error {
	bm.metadataLock.RLock()
	metadata, ok := bm.metadata[backupID]
	bm.metadataLock.RUnlock()

	if !ok {
		return errors.New("backup not found")
	}

	if metadata.Status != BackupStatusCompleted {
		return fmt.Errorf("backup not completed, status: %d", metadata.Status)
	}

	// è¯»å–æ–‡ä»¶
	compressed, err := os.ReadFile(metadata.FilePath)
	if err != nil {
		return err
	}

	// éªŒè¯æ ¡éªŒå’?
	if calculateChecksum(compressed) != metadata.Checksum {
		return errors.New("checksum verification failed")
	}

	// è§£å‹æ•°æ®
	jsonData, err := decompressData(compressed)
	if err != nil {
		return err
	}

	// ååºåˆ—åŒ–
	err = json.Unmarshal(jsonData, data)
	if err != nil {
		return err
	}

	return nil
}

// GetBackup è·å–å¤‡ä»½å…ƒæ•°æ?
func (bm *BackupManager) GetBackup(backupID string) (*BackupMetadata, error) {
	bm.metadataLock.RLock()
	defer bm.metadataLock.RUnlock()

	metadata, ok := bm.metadata[backupID]
	if !ok {
		return nil, errors.New("backup not found")
	}

	return metadata, nil
}

// ListBackups åˆ—å‡ºæ‰€æœ‰å¤‡ä»?
func (bm *BackupManager) ListBackups() []*BackupMetadata {
	bm.metadataLock.RLock()
	defer bm.metadataLock.RUnlock()

	backups := make([]*BackupMetadata, 0, len(bm.backups))
	for _, id := range bm.backups {
		if metadata, ok := bm.metadata[id]; ok {
			backups = append(backups, metadata)
		}
	}

	return backups
}

// DeleteBackup åˆ é™¤å¤‡ä»½
func (bm *BackupManager) DeleteBackup(backupID string) error {
	bm.metadataLock.Lock()
	defer bm.metadataLock.Unlock()

	metadata, ok := bm.metadata[backupID]
	if !ok {
		return errors.New("backup not found")
	}

	// åˆ é™¤æ–‡ä»¶
	if metadata.FilePath != "" {
		err := os.Remove(metadata.FilePath)
		if err != nil && !os.IsNotExist(err) {
			return err
		}
	}

	// åˆ é™¤å…ƒæ•°æ?
	delete(bm.metadata, backupID)

	// ä»åˆ—è¡¨ä¸­ç§»é™¤
	for i, id := range bm.backups {
		if id == backupID {
			bm.backups = append(bm.backups[:i], bm.backups[i+1:]...)
			break
		}
	}

	return nil
}

// CleanOldBackups æ¸…ç†æ—§å¤‡ä»?
func (bm *BackupManager) CleanOldBackups(olderThan time.Duration, keepCount int) error {
	bm.metadataLock.Lock()
	defer bm.metadataLock.Unlock()

	now := time.Now()
	toBeDeleted := make([]string, 0)

	// æ ‡è®°è¿‡æœŸçš„å¤‡ä»?
	for _, metadata := range bm.metadata {
		if now.Sub(metadata.EndTime) > olderThan {
			toBeDeleted = append(toBeDeleted, metadata.ID)
		}
	}

	// ä¿ç•™æœ€è¿‘çš„Nä¸ªå¤‡ä»?
	if len(toBeDeleted) > 0 && keepCount > 0 {
		// æŒ‰æ—¶é—´æ’åº?
		allBackups := make([]*BackupMetadata, 0, len(bm.metadata))
		for _, metadata := range bm.metadata {
			if metadata.Status == BackupStatusCompleted {
				allBackups = append(allBackups, metadata)
			}
		}

		// ç®€å•çš„å†’æ³¡æ’åºï¼ˆæŒ‰æ—¶é—´é™åºï¼?
		for i := 0; i < len(allBackups); i++ {
			for j := i + 1; j < len(allBackups); j++ {
				if allBackups[i].EndTime.Before(allBackups[j].EndTime) {
					allBackups[i], allBackups[j] = allBackups[j], allBackups[i]
				}
			}
		}

		// ä¿ç•™æœ€æ–°çš„
		keepSet := make(map[string]bool)
		for i := 0; i < keepCount && i < len(allBackups); i++ {
			keepSet[allBackups[i].ID] = true
		}

		// è¿‡æ»¤æ‰éœ€è¦ä¿ç•™çš„
		filtered := make([]string, 0, len(toBeDeleted))
		for _, id := range toBeDeleted {
			if !keepSet[id] {
				filtered = append(filtered, id)
			}
		}
		toBeDeleted = filtered
	}

	// åˆ é™¤å¤‡ä»½
	for _, id := range toBeDeleted {
		if metadata, ok := bm.metadata[id]; ok {
			if metadata.FilePath != "" {
				os.Remove(metadata.FilePath)
			}
			delete(bm.metadata, id)
		}
	}

	return nil
}

// GetBackupStats è·å–å¤‡ä»½ç»Ÿè®¡ä¿¡æ¯
func (bm *BackupManager) GetBackupStats() (totalBackups int, totalSize int64, err error) {
	bm.metadataLock.RLock()
	defer bm.metadataLock.RUnlock()

	for _, metadata := range bm.metadata {
		if metadata.Status == BackupStatusCompleted {
			totalBackups++
			totalSize += metadata.Size
		}
	}

	return totalBackups, totalSize, nil
}

// compressData å‹ç¼©æ•°æ®
func compressData(data []byte) ([]byte, error) {
	var buf bytes.Buffer
	writer := gzip.NewWriter(&buf)

	_, err := writer.Write(data)
	if err != nil {
		return nil, err
	}

	err = writer.Close()
	if err != nil {
		return nil, err
	}

	return buf.Bytes(), nil
}

// decompressData è§£å‹æ•°æ®
func decompressData(compressed []byte) ([]byte, error) {
	reader, err := gzip.NewReader(bytes.NewReader(compressed))
	if err != nil {
		return nil, err
	}
	defer reader.Close()

	data, err := io.ReadAll(reader)
	if err != nil {
		return nil, err
	}

	return data, nil
}

// generateBackupID ç”Ÿæˆå¤‡ä»½ID
func generateBackupID() string {
	return fmt.Sprintf("backup_%d", time.Now().UnixNano())
}

// calculateRecordCount è®¡ç®—è®°å½•æ•?
func calculateRecordCount(data interface{}) int {
	if data == nil {
		return 0
	}

	// ç®€åŒ–å®ç°ï¼Œå®é™…åº”è¯¥æ ¹æ®æ•°æ®ç±»å‹è®¡ç®—
	switch v := data.(type) {
	case map[string]interface{}:
		return len(v)
	case []interface{}:
		return len(v)
	case map[string][]interface{}:
		count := 0
		for _, rows := range v {
			count += len(rows)
		}
		return count
	default:
		return 1
	}
}

// calculateChecksum è®¡ç®—æ ¡éªŒå’?
func calculateChecksum(data []byte) string {
	sum := 0
	for _, b := range data {
		sum += int(b)
	}
	return fmt.Sprintf("%x", sum)
}

// ExportMetadata å¯¼å‡ºå¤‡ä»½å…ƒæ•°æ?
func (bm *BackupManager) ExportMetadata() ([]byte, error) {
	bm.metadataLock.RLock()
	defer bm.metadataLock.RUnlock()

	return json.MarshalIndent(bm.metadata, "", "  ")
}

// ImportMetadata å¯¼å…¥å¤‡ä»½å…ƒæ•°æ?
func (bm *BackupManager) ImportMetadata(data []byte) error {
	bm.metadataLock.Lock()
	defer bm.metadataLock.Unlock()

	metadataMap := make(map[string]*BackupMetadata)
	err := json.Unmarshal(data, &metadataMap)
	if err != nil {
		return err
	}

	bm.metadata = metadataMap
	bm.backups = make([]string, 0, len(metadataMap))
	for id := range metadataMap {
		bm.backups = append(bm.backups, id)
	}

	return nil
}
