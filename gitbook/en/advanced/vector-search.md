# Vector Search

SQLExec supports Approximate Nearest Neighbor (ANN) search for high-dimensional vectors, suitable for AI application scenarios such as semantic search, recommendation systems, and image retrieval.

## Vector Column Type

Use the `VECTOR(dim)` type to define a vector column, where `dim` is the vector dimension.

```sql
CREATE TABLE documents (
    id INT PRIMARY KEY,
    title VARCHAR(200),
    content TEXT,
    embedding VECTOR(768)
);
```

## Vector Index Types

SQLExec provides 10 vector index types, covering a range of scenarios from exact search to extreme compression:

| Index Type | Description | Accuracy | Speed | Memory Usage | Use Case |
|---------|------|------|------|---------|---------|
| Flat | Brute-force search, comparing one by one | Perfect | Slow | High | Small datasets, accuracy benchmarks |
| IVF-Flat | IVF clustering + exact distance | Good | Fast | Medium | Medium-scale datasets |
| IVF-SQ8 | IVF + scalar quantization | Fairly good | Fast | Fairly low | Memory-constrained scenarios |
| IVF-PQ | IVF + product quantization | Moderate | Fast | Low | Large-scale datasets |
| **HNSW** | Hierarchical Navigable Small World graph | **Excellent** | **Fast** | Medium | **Recommended, general-purpose** |
| HNSW-SQ | HNSW + scalar quantization | Good | Fast | Fairly low | Balancing accuracy and memory |
| HNSW-PQ | HNSW + product quantization | Fairly good | Fast | Low | Large-scale high-dimensional data |
| HNSW-PRQ | HNSW + progressive residual quantization | Good | Fast | Low | High compression ratio scenarios |
| IVF-RabitQ | IVF + RabitQ quantization | Fairly good | Fast | Low | Very large-scale datasets |
| AISAQ | Asymmetric ISAQ | Fairly good | Fast | Very low | Extremely memory-constrained scenarios |

> **Recommendation**: For most scenarios, the **HNSW** index is recommended as it achieves the best balance between accuracy and speed.

## Creating a Vector Index

```sql
CREATE VECTOR INDEX idx_embedding ON documents(embedding)
    USING HNSW
    WITH (
        metric = 'cosine',
        dim = 768,
        m = 16,
        ef_construction = 200
    );
```

### Index Parameter Reference

| Parameter | Description | Default |
|------|------|--------|
| `metric` | Distance metric | `cosine` |
| `dim` | Vector dimension | Same as column definition |
| `m` | Maximum number of connections per node in HNSW | `16` |
| `ef_construction` | Search width during HNSW construction | `200` |
| `nprobe` | Number of clusters to probe during IVF search | `10` |
| `pq_m` | Number of subspaces for PQ product quantization | `8` |

## Distance Metrics

SQLExec supports 3 distance metrics:

| Metric | Function | Range | Description |
|---------|------|------|------|
| Cosine | `vec_cosine_distance(a, b)` | [0, 2] | Cosine distance, suitable for normalized vectors; smaller values indicate greater similarity |
| L2 / Euclidean | `vec_l2_distance(a, b)` | [0, +∞) | Euclidean distance, suitable for raw vectors |
| Inner Product | `vec_ip_distance(a, b)` | (-∞, +∞) | Inner product distance; larger values indicate greater similarity |

## Top-K Queries

Use `ORDER BY` with a distance function combined with `LIMIT` to perform Top-K nearest neighbor search:

```sql
SELECT id, title,
       vec_cosine_distance(embedding, '[0.1, 0.2, ..., 0.5]') AS distance
FROM documents
ORDER BY vec_cosine_distance(embedding, '[0.1, 0.2, ..., 0.5]')
LIMIT 10;
```

## Hybrid Search

Combine vector search scores with full-text search scores for more accurate semantic + keyword hybrid retrieval:

```sql
SELECT id, title,
       vec_cosine_distance(embedding, '[0.1, 0.2, ..., 0.5]') AS vec_score,
       MATCH(content) AGAINST('机器学习') AS text_score,
       vec_cosine_distance(embedding, '[0.1, 0.2, ..., 0.5]') * 0.7
           + (1 - MATCH(content) AGAINST('机器学习')) * 0.3 AS combined_score
FROM documents
WHERE MATCH(content) AGAINST('机器学习')
ORDER BY combined_score
LIMIT 10;
```

## Complete Example

Here is a complete vector search workflow:

```sql
-- 1. Create a table with a vector column
CREATE TABLE articles (
    id INT PRIMARY KEY,
    title VARCHAR(200),
    content TEXT,
    embedding VECTOR(384)
);

-- 2. Insert data (vectors are typically generated by external models)
INSERT INTO articles (id, title, content, embedding) VALUES
(1, '深度学习入门', '神经网络是深度学习的基础...', '[0.12, 0.45, ..., 0.78]'),
(2, '自然语言处理', 'NLP 是 AI 的重要分支...', '[0.34, 0.67, ..., 0.91]'),
(3, '计算机视觉', '图像识别技术广泛应用...', '[0.56, 0.23, ..., 0.44]');

-- 3. Create an HNSW vector index
CREATE VECTOR INDEX idx_article_emb ON articles(embedding)
    USING HNSW
    WITH (metric = 'cosine', m = 16, ef_construction = 200);

-- 4. Search for the most similar articles
SELECT id, title,
       vec_cosine_distance(embedding, '[0.13, 0.44, ..., 0.80]') AS distance
FROM articles
ORDER BY vec_cosine_distance(embedding, '[0.13, 0.44, ..., 0.80]')
LIMIT 5;
```
